from fastapi import FastAPI, UploadFile, File, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import json
import os
import uuid
import re
from typing import List, Optional
from crawler_service import crawler_service

app = FastAPI(title="å®¶å…·ç»´ä¿®æ™ºèƒ½åŠ©æ‰‹", description="åŸºäºRAGçš„å®¶å…·ç»´ä¿®çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class QARequest(BaseModel):
    query: str
    answer_mode: str = "auto"  # auto, llm_only, kb_only
    model: str = "gpt-3.5-turbo"
    temperature: float = 0.7
    context_size: int = 3

class QAResponse(BaseModel):
    answer: str
    sources: Optional[List[str]] = None
    confidence: Optional[float] = None
    related_questions: Optional[List[str]] = []
    model_used: Optional[str] = None
    answer_mode: str = "auto"
    processing_time: Optional[float] = None

class UploadResponse(BaseModel):
    success: bool
    message: str
    file_id: Optional[str] = None

class CollectRequest(BaseModel):
    url: str

class ChatHistory(BaseModel):
    question: str
    answer: str
    timestamp: str

# æ–°å¢æ¨¡å‹é…ç½®
class ModelConfig(BaseModel):
    model_name: str
    temperature: float = 0.7
    max_tokens: int = 1000

# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
SUPPORTED_MODELS = {
    "gpt-3.5-turbo": {
        "name": "GPT-3.5 Turbo",
        "provider": "OpenAI",
        "description": "å¿«é€Ÿã€é«˜æ•ˆçš„å¯¹è¯æ¨¡å‹"
    },
    "gpt-4": {
        "name": "GPT-4",
        "provider": "OpenAI", 
        "description": "æ›´å¼ºå¤§çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›"
    },
    "chatglm_std": {
        "name": "ChatGLM æ ‡å‡†ç‰ˆ",
        "provider": "æ™ºè°±AI",
        "description": "ä¸­æ–‡ä¼˜åŒ–çš„å¯¹è¯æ¨¡å‹"
    },
    "chatglm_pro": {
        "name": "ChatGLM Pro",
        "provider": "æ™ºè°±AI",
        "description": "ä¸“ä¸šçº§ä¸­æ–‡å¯¹è¯æ¨¡å‹"
    },
    "ERNIE-Bot": {
        "name": "æ–‡å¿ƒä¸€è¨€",
        "provider": "ç™¾åº¦",
        "description": "ç™¾åº¦è‡ªç ”çš„å¤§è¯­è¨€æ¨¡å‹"
    }
}

# å…¨å±€å˜é‡å­˜å‚¨çŸ¥è¯†åº“å’Œå¯¹è¯å†å²
knowledge_base = []
chat_history = []

def load_knowledge_base():
    """åŠ è½½å·²æœ‰çš„ç»´ä¿®æ•°æ®"""
    global knowledge_base
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®æ–‡ä»¶è·¯å¾„
    possible_paths = [
        "data/raw/phone.json",
        "../data/raw/phone.json", 
        "our_data/phone.json",
        "../our_data/phone.json",
        "data/phone.json",
        "../data/phone.json"
    ]
    
    phone_json_file = None
    for path in possible_paths:
        if os.path.exists(path):
            phone_json_file = path
            print(f"ğŸ“‚ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {path}")
            break
    
    if not phone_json_file:
        print("âš ï¸  æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œçˆ¬è™«æŠ“å–æ•°æ®")
        print("ğŸ’¡ æç¤º: è¿›å…¥ backend/data/raw/ ç›®å½•è¿è¡Œ:")
        print("   python quick_crawl.py")
        create_comprehensive_sample_data()
        phone_json_file = "data/raw/phone.json"
    
    try:
        with open(phone_json_file, 'r', encoding='utf-8') as f:
            phone_data = json.load(f)
            print(f"ğŸ“± åŸå§‹æ•°æ®æ¡æ•°: {len(phone_data)}")
            
            for i, item in enumerate(phone_data):
                content = item.get('content', '')
                title = item.get('title', '')
                
                # é™ä½è¿‡æ»¤æ¡ä»¶ï¼Œä¿ç•™æ›´å¤šæ•°æ®
                if content and len(content.strip()) > 50:
                    # æ¸…æ´—å’Œç»“æ„åŒ–å†…å®¹
                    structured_content = structure_repair_content(content)
                    knowledge_base.append({
                        'content': structured_content,
                        'title': title,
                        'url': item.get('url', ''),
                        'type': 'phone_repair',
                        'keywords': extract_keywords(content)
                    })
                    if i < 5:  # åªæ˜¾ç¤ºå‰5æ¡çš„å¤„ç†ä¿¡æ¯
                        print(f"âœ… å¤„ç†ç¬¬ {i+1} æ¡: {title[:50]}...")
                else:
                    if i < 5:
                        print(f"âŒ è·³è¿‡ç¬¬ {i+1} æ¡: å†…å®¹å¤ªçŸ­ ({len(content)} å­—ç¬¦)")
        
        print(f"ğŸ“± æˆåŠŸåŠ è½½ç»´ä¿®æ•°æ®: {len(knowledge_base)} æ¡")
        
        # å¦‚æœæ•°æ®é‡è¿˜æ˜¯å¾ˆå°‘ï¼Œæç¤ºç”¨æˆ·
        if len(knowledge_base) < 10:
            print("\nğŸ’¡ æ•°æ®é‡è¾ƒå°‘çš„è§£å†³æ–¹æ¡ˆ:")
            print("1. è¿›å…¥ backend/data/raw/ ç›®å½•")
            print("2. è¿è¡Œ: python quick_crawl.py")
            print("3. é€‰æ‹©æŠ“å–æ›´å¤šURL (å»ºè®®50-100ä¸ª)")
            print("4. é‡å¯åç«¯æœåŠ¡")
        
    except Exception as e:
        print(f"âŒ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
        create_comprehensive_sample_data()

def structure_repair_content(content: str) -> dict:
    """ç»“æ„åŒ–ç»´ä¿®å†…å®¹"""
    # æå–æ­¥éª¤
    steps = extract_repair_steps(content)
    
    # æå–å·¥å…·åˆ—è¡¨
    tools = extract_tools(content)
    
    # æå–æ³¨æ„äº‹é¡¹
    warnings = extract_warnings(content)
    
    # æå–é›¶ä»¶ä¿¡æ¯
    parts = extract_parts(content)
    
    return {
        'raw_content': content,
        'steps': steps,
        'tools': tools,
        'warnings': warnings,
        'parts': parts,
        'summary': content[:300] + "..." if len(content) > 300 else content
    }

def extract_repair_steps(content: str) -> List[str]:
    """æå–ç»´ä¿®æ­¥éª¤"""
    step_patterns = [
        r'æ­¥éª¤\s*\d+[ï¼š:]\s*(.+?)(?=æ­¥éª¤\s*\d+|$)',
        r'Step\s*\d+[ï¼š:]\s*(.+?)(?=Step\s*\d+|$)',
        r'\d+[\.ã€]\s*(.+?)(?=\d+[\.ã€]|$)'
    ]
    
    steps = []
    for pattern in step_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
        if matches:
            steps.extend([step.strip() for step in matches if len(step.strip()) > 10])
            break
    
    return steps[:10]  # æœ€å¤šè¿”å›10ä¸ªæ­¥éª¤

def extract_tools(content: str) -> List[str]:
    """æå–å·¥å…·åˆ—è¡¨"""
    tool_keywords = [
        'screwdriver', 'spudger', 'tweezers', 'opening pick', 'suction handle',
        'èºä¸åˆ€', 'æ’¬æ£’', 'é•Šå­', 'æ’¬ç‰‡', 'å¸ç›˜', 'çƒ­é£æª', 'heat gun', 'hair dryer'
    ]
    
    tools = []
    for keyword in tool_keywords:
        if keyword.lower() in content.lower():
            tools.append(keyword)
    
    return list(set(tools))

def extract_warnings(content: str) -> List[str]:
    """æå–æ³¨æ„äº‹é¡¹"""
    warning_patterns = [
        r'æ³¨æ„[ï¼š:](.+?)(?=\n|$)',
        r'å°å¿ƒ[ï¼š:](.+?)(?=\n|$)',
        r'Be careful(.+?)(?=\n|$)',
        r'Don\'t(.+?)(?=\n|$)'
    ]
    
    warnings = []
    for pattern in warning_patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        warnings.extend([warning.strip() for warning in matches])
    
    return warnings[:5]  # æœ€å¤šè¿”å›5ä¸ªè­¦å‘Š

def extract_parts(content: str) -> List[str]:
    """æå–é›¶ä»¶ä¿¡æ¯"""
    part_keywords = [
        'battery', 'screen', 'camera', 'speaker', 'antenna', 'microphone',
        'ç”µæ± ', 'å±å¹•', 'æ‘„åƒå¤´', 'æ‰¬å£°å™¨', 'å¤©çº¿', 'éº¦å…‹é£', 'åç›–', 'å……ç”µå£'
    ]
    
    parts = []
    for keyword in part_keywords:
        if keyword.lower() in content.lower():
            parts.append(keyword)
    
    return list(set(parts))

def extract_keywords(content: str) -> List[str]:
    """æå–å…³é”®è¯"""
    # ç»´ä¿®ç›¸å…³å…³é”®è¯
    repair_keywords = [
        'replacement', 'repair', 'fix', 'install', 'remove', 'disconnect',
        'æ›´æ¢', 'ç»´ä¿®', 'ä¿®ç†', 'å®‰è£…', 'ç§»é™¤', 'æ–­å¼€', 'è¿æ¥', 'æ‹†è§£'
    ]
    
    keywords = []
    content_lower = content.lower()
    
    for keyword in repair_keywords:
        if keyword.lower() in content_lower:
            keywords.append(keyword)
    
    return keywords

def enhanced_search(query: str, top_k: int = 3) -> List[dict]:
    """å¢å¼ºçš„æœç´¢åŠŸèƒ½"""
    query_lower = query.lower()
    results = []
    
    for item in knowledge_base:
        score = 0
        content = item['content']
        
        # å…³é”®è¯åŒ¹é…
        for keyword in item.get('keywords', []):
            if keyword.lower() in query_lower:
                score += 2
        
        # å†…å®¹åŒ¹é…
        if isinstance(content, dict):
            # åœ¨æ‘˜è¦ä¸­æœç´¢
            if any(word in content.get('summary', '').lower() for word in query_lower.split()):
                score += 1
            
            # åœ¨æ­¥éª¤ä¸­æœç´¢
            for step in content.get('steps', []):
                if any(word in step.lower() for word in query_lower.split()):
                    score += 1.5
        else:
            # ç›´æ¥å†…å®¹æœç´¢
            if any(word in content.lower() for word in query_lower.split()):
                score += 1
        
        if score > 0:
            results.append({
                'item': item,
                'score': score
            })
    
    # æŒ‰åˆ†æ•°æ’åº
    results.sort(key=lambda x: x['score'], reverse=True)
    return [result['item'] for result in results[:top_k]]

def generate_detailed_answer(query: str, contexts: List[dict]) -> str:
    """ç”Ÿæˆè¯¦ç»†çš„å›ç­”"""
    if not contexts:
        return "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç»´ä¿®ä¿¡æ¯ã€‚è¯·å°è¯•æ›´å…·ä½“çš„é—®é¢˜ï¼Œæ¯”å¦‚'å¦‚ä½•æ›´æ¢iPhoneç”µæ± 'æˆ–'å±å¹•ç»´ä¿®æ­¥éª¤'ã€‚"
    
    answer_parts = []
    answer_parts.append(f"æ ¹æ®ç»´ä¿®çŸ¥è¯†åº“ï¼Œå…³äº'{query}'çš„è§£ç­”å¦‚ä¸‹ï¼š\n")
    
    for i, context in enumerate(contexts[:2], 1):
        content = context['content']
        
        if isinstance(content, dict):
            # ç»“æ„åŒ–å†…å®¹
            answer_parts.append(f"ğŸ“‹ **æ–¹æ³• {i}ï¼š{context.get('title', 'ç»´ä¿®æŒ‡å—')}**\n")
            
            # æ·»åŠ æ­¥éª¤
            if content.get('steps'):
                answer_parts.append("ğŸ”§ **ä¸»è¦æ­¥éª¤ï¼š**")
                for j, step in enumerate(content['steps'][:3], 1):
                    answer_parts.append(f"   {j}. {step[:100]}...")
                answer_parts.append("")
            
            # æ·»åŠ å·¥å…·
            if content.get('tools'):
                tools_str = "ã€".join(content['tools'][:5])
                answer_parts.append(f"ğŸ› ï¸ **æ‰€éœ€å·¥å…·ï¼š** {tools_str}\n")
            
            # æ·»åŠ æ³¨æ„äº‹é¡¹
            if content.get('warnings'):
                answer_parts.append("âš ï¸ **æ³¨æ„äº‹é¡¹ï¼š**")
                for warning in content['warnings'][:2]:
                    answer_parts.append(f"   â€¢ {warning}")
                answer_parts.append("")
        else:
            # ç®€å•å†…å®¹
            answer_parts.append(f"**å‚è€ƒä¿¡æ¯ {i}ï¼š**\n")
            answer_parts.append(content[:200] + "...\n")
    
    answer_parts.append("ğŸ’¡ **æ¸©é¦¨æç¤ºï¼š** ç»´ä¿®è¿‡ç¨‹ä¸­è¯·åŠ¡å¿…æ–­ç”µæ“ä½œï¼Œå¦‚é‡å›°éš¾å»ºè®®å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚")
    
    return "\n".join(answer_parts)

def generate_related_questions(query: str, contexts: List[dict]) -> List[str]:
    """ç”Ÿæˆç›¸å…³é—®é¢˜"""
    related_questions = []
    
    # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç›¸å…³é—®é¢˜
    for context in contexts:
        if isinstance(context['content'], dict):
            parts = context['content'].get('parts', [])
            for part in parts[:2]:
                related_questions.append(f"å¦‚ä½•æ›´æ¢{part}ï¼Ÿ")
                related_questions.append(f"{part}ç»´ä¿®éœ€è¦ä»€ä¹ˆå·¥å…·ï¼Ÿ")
    
    # é€šç”¨ç›¸å…³é—®é¢˜
    common_questions = [
        "ç»´ä¿®å‰éœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ",
        "å¸¸ç”¨çš„ç»´ä¿®å·¥å…·æœ‰å“ªäº›ï¼Ÿ",
        "å¦‚ä½•åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ¢é›¶ä»¶ï¼Ÿ",
        "ç»´ä¿®åå¦‚ä½•æµ‹è¯•åŠŸèƒ½ï¼Ÿ"
    ]
    
    related_questions.extend(common_questions)
    return list(set(related_questions))[:4]

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    os.makedirs("data/raw", exist_ok=True)
    
    sample_data = [
        {
            "url": "sample://battery-replacement",
            "title": "iPhoneç”µæ± æ›´æ¢æŒ‡å—", 
            "content": """iPhoneç”µæ± æ›´æ¢æ­¥éª¤ï¼š
            
æ­¥éª¤ 1: å…³æœºå¹¶å‡†å¤‡å·¥å…·
å…³é—­iPhoneç”µæºï¼Œå‡†å¤‡P2äº”è§’èºä¸åˆ€ã€æ’¬æ£’ã€é•Šå­ç­‰å·¥å…·ã€‚

æ­¥éª¤ 2: ç§»é™¤èºä¸
ä½¿ç”¨P2èºä¸åˆ€ç§»é™¤å……ç”µå£ä¸¤ä¾§çš„èºä¸ã€‚

æ­¥éª¤ 3: æ‰“å¼€åç›–
ç”¨å¸ç›˜å’Œæ’¬ç‰‡å°å¿ƒæ‰“å¼€åç›–ã€‚

æ­¥éª¤ 4: æ–­å¼€ç”µæ± 
ä½¿ç”¨æ’¬æ£’æ–­å¼€ç”µæ± è¿æ¥å™¨ã€‚

æ­¥éª¤ 5: ç§»é™¤èƒ¶æ¡
æ’•æ‰ç”µæ± ä¸‹æ–¹çš„æ‹‰æ¡èƒ¶å¸¦ã€‚

æ­¥éª¤ 6: å®‰è£…æ–°ç”µæ± 
æ”¾å…¥æ–°ç”µæ± å¹¶é‡æ–°è¿æ¥ã€‚

æ‰€éœ€å·¥å…·: P2èºä¸åˆ€ã€æ’¬æ£’ã€é•Šå­ã€å¸ç›˜
æ³¨æ„: æ“ä½œæ—¶è¯·æ–­ç”µï¼Œå°å¿ƒé™ç”µ"""
        }
    ]
    
    with open("data/raw/phone.json", 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)

def create_comprehensive_sample_data():
    """åˆ›å»ºæ›´å…¨é¢çš„ç¤ºä¾‹æ•°æ®"""
    os.makedirs("data/raw", exist_ok=True)
    
    sample_data = [
        {
            "url": "sample://battery-replacement",
            "title": "iPhoneç”µæ± æ›´æ¢æŒ‡å—", 
            "content": """iPhoneç”µæ± æ›´æ¢å®Œæ•´æŒ‡å—

ç®€ä»‹
æœ¬æŒ‡å—å°†æ•™ä½ å¦‚ä½•å®‰å…¨åœ°æ›´æ¢iPhoneç”µæ± ã€‚å¦‚æœä½ çš„iPhoneç”µæ± ç»­èˆªæ—¶é—´æ˜æ˜¾ç¼©çŸ­ï¼Œæˆ–è€…ç³»ç»Ÿæç¤ºç”µæ± å¥åº·åº¦ä½äº80%ï¼Œé‚£ä¹ˆå¯èƒ½éœ€è¦æ›´æ¢ç”µæ± ã€‚

ä½ æ‰€éœ€è¦çš„å·¥å…·
- P2äº”è§’èºä¸åˆ€
- æ’¬æ£’ (spudger)
- é•Šå­ (tweezers)
- å¸ç›˜ (suction handle)
- æ’¬ç‰‡ (opening pick)
- çƒ­é£æªæˆ–å¹é£æœº

æ­¥éª¤ 1: å‡†å¤‡å·¥ä½œ
å…³é—­iPhoneç”µæºï¼Œç¡®ä¿ç”µæ± ç”µé‡ä½äº25%ä»¥é™ä½å®‰å…¨é£é™©ã€‚

æ­¥éª¤ 2: ç§»é™¤èºä¸
ä½¿ç”¨P2äº”è§’èºä¸åˆ€ç§»é™¤å……ç”µå£ä¸¤ä¾§çš„ä¸¤é¢—èºä¸ã€‚

æ­¥éª¤ 3: æ‰“å¼€åç›–
ç”¨å¸ç›˜å’Œæ’¬ç‰‡å°å¿ƒæ‰“å¼€åç›–ï¼Œæ³¨æ„ä¸è¦æŸåæ’çº¿ã€‚

æ­¥éª¤ 4: æ–­å¼€ç”µæ± è¿æ¥
ä½¿ç”¨æ’¬æ£’æ–­å¼€ç”µæ± è¿æ¥å™¨ï¼Œç¡®ä¿å®Œå…¨æ–­ç”µã€‚

æ­¥éª¤ 5: ç§»é™¤èƒ¶æ¡
æ’•æ‰ç”µæ± ä¸‹æ–¹çš„æ‹‰æ¡èƒ¶å¸¦ï¼Œå¦‚æœæ–­è£‚å¯ä½¿ç”¨å¼‚ä¸™é†‡è½¯åŒ–ã€‚

æ­¥éª¤ 6: å®‰è£…æ–°ç”µæ± 
æ”¾å…¥æ–°ç”µæ± å¹¶é‡æ–°è¿æ¥æ’çº¿ã€‚

æ³¨æ„äº‹é¡¹ï¼š
- æ“ä½œæ—¶è¯·æ–­ç”µï¼Œå°å¿ƒé™ç”µ
- ä¸è¦ç”¨é‡‘å±å·¥å…·ç›´æ¥æ¥è§¦ç”µæ± 
- å¤„ç†å¥½æ—§ç”µæ± çš„å›æ”¶"""
        },
        {
            "url": "sample://screen-replacement",
            "title": "iPhoneå±å¹•æ›´æ¢æŒ‡å—",
            "content": """iPhoneå±å¹•æ›´æ¢è¯¦ç»†æ­¥éª¤

ç®€ä»‹
å¦‚æœä½ çš„iPhoneå±å¹•ç ´è£‚æˆ–æ˜¾ç¤ºå¼‚å¸¸ï¼Œæœ¬æŒ‡å—å°†å¸®ä½ å®Œæˆå±å¹•æ›´æ¢ã€‚

å·¥å…·æ¸…å•
- çƒ­é£æª (heat gun)
- æ’¬ç‰‡ (opening pick)
- å¸ç›˜
- èºä¸åˆ€å¥—è£…
- é•Šå­

æ­¥éª¤ 1: åŠ çƒ­å±å¹•
ä½¿ç”¨çƒ­é£æªåŠ çƒ­å±å¹•è¾¹ç¼˜ï¼Œè½¯åŒ–å¯†å°èƒ¶ã€‚

æ­¥éª¤ 2: åˆ›å»ºç¼éš™
ç”¨å¸ç›˜æ‹‰èµ·å±å¹•ä¸€è§’ï¼Œæ’å…¥æ’¬ç‰‡ã€‚

æ­¥éª¤ 3: åˆ†ç¦»å±å¹•
æ²¿ç€è¾¹ç¼˜æ»‘åŠ¨æ’¬ç‰‡ï¼Œåˆ†ç¦»å±å¹•ç»„ä»¶ã€‚

æ­¥éª¤ 4: æ–­å¼€æ’çº¿
å°å¿ƒæ–­å¼€å±å¹•æ’çº¿è¿æ¥å™¨ã€‚

æ­¥éª¤ 5: å®‰è£…æ–°å±å¹•
æŒ‰ç›¸åé¡ºåºå®‰è£…æ–°å±å¹•ç»„ä»¶ã€‚

è­¦å‘Šï¼š
- åŠ çƒ­æ—¶ä¸è¦è¿‡åº¦ï¼Œé¿å…æŸåå†…éƒ¨ç»„ä»¶
- æ’çº¿éå¸¸è„†å¼±ï¼Œæ“ä½œè¦è½»æŸ”"""
        },
        {
            "url": "sample://camera-repair",
            "title": "æ‘„åƒå¤´ç»´ä¿®æŒ‡å—",
            "content": """iPhoneæ‘„åƒå¤´ç»´ä¿®æŒ‡å—

æ•…éšœè¯Šæ–­
- æ‘„åƒå¤´æ¨¡ç³Šä¸æ¸…
- é»‘å±æˆ–æ— æ³•å¯åŠ¨
- é—ªå…‰ç¯ä¸å·¥ä½œ
- è‡ªåŠ¨å¯¹ç„¦å¤±æ•ˆ

ç»´ä¿®æ­¥éª¤
æ­¥éª¤ 1: æ£€æŸ¥è½¯ä»¶é—®é¢˜
é‡å¯è®¾å¤‡ï¼Œæ£€æŸ¥ç›¸æœºåº”ç”¨è®¾ç½®ã€‚

æ­¥éª¤ 2: æ¸…æ´é•œå¤´
ä½¿ç”¨ä¸“ç”¨æ¸…æ´å¸ƒæ“¦æ‹­é•œå¤´è¡¨é¢ã€‚

æ­¥éª¤ 3: æ›´æ¢æ‘„åƒå¤´æ¨¡å—
å¦‚éœ€æ›´æ¢ç¡¬ä»¶ï¼Œéœ€è¦æ‹†è§£è®¾å¤‡ã€‚

æ‰€éœ€å·¥å…·ï¼š
- ç²¾å¯†èºä¸åˆ€
- é˜²é™ç”µæ‰‹å¥—
- æ¸…æ´å¸ƒ

æ³¨æ„ï¼šæ‘„åƒå¤´æ¨¡å—åŒ…å«ç²¾å¯†å…‰å­¦å…ƒä»¶ï¼Œæ“ä½œéœ€æ ¼å¤–å°å¿ƒã€‚"""
        },
        {
            "url": "sample://charging-port-repair", 
            "title": "å……ç”µå£ç»´ä¿®æŒ‡å—",
            "content": """å……ç”µå£é—®é¢˜è¯Šæ–­ä¸ç»´ä¿®

å¸¸è§é—®é¢˜
- å……ç”µé€Ÿåº¦æ…¢
- æ— æ³•å……ç”µ
- æ¥è§¦ä¸è‰¯
- å……ç”µå£æ¾åŠ¨

ç»´ä¿®æ–¹æ³•
æ­¥éª¤ 1: æ¸…æ´å……ç”µå£
ä½¿ç”¨å°åˆ·å­æ¸…é™¤ç°å°˜å’Œå¼‚ç‰©ã€‚

æ­¥éª¤ 2: æ£€æŸ¥å……ç”µçº¿
æµ‹è¯•ä¸åŒçš„å……ç”µçº¿å’Œé€‚é…å™¨ã€‚

æ­¥éª¤ 3: æ›´æ¢å……ç”µå£ç»„ä»¶
å¦‚éœ€æ›´æ¢ç¡¬ä»¶ï¼ŒæŒ‰ä»¥ä¸‹æ­¥éª¤ï¼š
- æ‹†å¸åº•éƒ¨èºä¸
- æ–­å¼€ç›¸å…³æ’çº¿
- æ›´æ¢å……ç”µå£æ¨¡å—

å·¥å…·è¦æ±‚ï¼š
- P2èºä¸åˆ€
- æ’¬æ£’
- æ–°çš„å……ç”µå£ç»„ä»¶"""
        },
        {
            "url": "sample://speaker-repair",
            "title": "æ‰¬å£°å™¨ç»´ä¿®æŒ‡å—", 
            "content": """iPhoneæ‰¬å£°å™¨ç»´ä¿®æŒ‡å—

æ•…éšœç°è±¡
- æ— å£°éŸ³è¾“å‡º
- å£°éŸ³ç ´éŸ³æˆ–å¤±çœŸ
- éŸ³é‡å¾ˆå°
- åªæœ‰ä¸€ä¸ªæ‰¬å£°å™¨å·¥ä½œ

è¯Šæ–­æ­¥éª¤
æ­¥éª¤ 1: è½¯ä»¶æ£€æŸ¥
æ£€æŸ¥éŸ³é‡è®¾ç½®ã€é™éŸ³å¼€å…³ã€è“ç‰™è¿æ¥ã€‚

æ­¥éª¤ 2: ç¡¬ä»¶æ£€æŸ¥
æµ‹è¯•ä¸åŒçš„éŸ³é¢‘æºå’Œåº”ç”¨ã€‚

æ­¥éª¤ 3: æ¸…æ´æ‰¬å£°å™¨
æ¸…é™¤æ‰¬å£°å™¨ç½‘ç½©çš„ç°å°˜å’Œå¼‚ç‰©ã€‚

æ­¥éª¤ 4: æ›´æ¢æ‰¬å£°å™¨
- æ‹†å¸ç›¸å…³èºä¸
- æ–­å¼€æ‰¬å£°å™¨æ’çº¿
- å®‰è£…æ–°æ‰¬å£°å™¨

æ‰€éœ€å·¥å…·ï¼š
- èºä¸åˆ€
- æ’¬æ£’
- é•Šå­
- æ–°æ‰¬å£°å™¨ç»„ä»¶"""
        }
    ]
    
    with open("data/raw/phone.json", 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… åˆ›å»ºäº† {len(sample_data)} æ¡ç¤ºä¾‹ç»´ä¿®æ•°æ®")

def get_available_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥é€»è¾‘
        # ä¾‹å¦‚æ£€æŸ¥APIå¯†é’¥æ˜¯å¦é…ç½®ç­‰
        available = []
        for model_id, info in SUPPORTED_MODELS.items():
            # ç®€å•æ£€æŸ¥ - å®é™…é¡¹ç›®ä¸­åº”è¯¥æ£€æŸ¥APIå¯†é’¥ç­‰
            available.append({
                "id": model_id,
                "name": info["name"],
                "provider": info["provider"],
                "description": info["description"],
                "available": True  # è¿™é‡Œåº”è¯¥æ˜¯çœŸå®çš„å¯ç”¨æ€§æ£€æŸ¥
            })
        return available
    except Exception as e:
        print(f"æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§å¤±è´¥: {e}")
        return []

def call_llm_model(model_name: str, prompt: str, temperature: float = 0.7) -> str:
    """è°ƒç”¨æŒ‡å®šçš„å¤§è¯­è¨€æ¨¡å‹"""
    try:
        # è¿™é‡Œé›†æˆä½ ç°æœ‰çš„æ¨¡å‹è°ƒç”¨ä»£ç 
        # æ ¹æ®model_nameé€‰æ‹©ä¸åŒçš„æ¨¡å‹æ¥å£
        
        if model_name.startswith("gpt-"):
            # OpenAIæ¨¡å‹è°ƒç”¨
            return call_openai_model(model_name, prompt, temperature)
        elif model_name.startswith("chatglm"):
            # æ™ºè°±AIæ¨¡å‹è°ƒç”¨
            return call_zhipu_model(model_name, prompt, temperature)
        elif model_name.startswith("ERNIE"):
            # ç™¾åº¦æ–‡å¿ƒæ¨¡å‹è°ƒç”¨
            return call_wenxin_model(model_name, prompt, temperature)
        else:
            # é»˜è®¤ä½¿ç”¨æ¨¡æ‹Ÿå›å¤
            return generate_mock_response(prompt)
            
    except Exception as e:
        print(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        return f"æŠ±æ­‰ï¼Œæ¨¡å‹è°ƒç”¨å‡ºç°é—®é¢˜ï¼š{str(e)}"

def call_openai_model(model_name: str, prompt: str, temperature: float) -> str:
    """è°ƒç”¨OpenAIæ¨¡å‹"""
    try:
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä½ ç°æœ‰çš„OpenAIè°ƒç”¨ä»£ç 
        # import openai
        # response = openai.ChatCompletion.create(...)
        return f"[OpenAI {model_name}] è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›å¤ï¼š{prompt[:50]}..."
    except Exception as e:
        raise Exception(f"OpenAIæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")

def call_zhipu_model(model_name: str, prompt: str, temperature: float) -> str:
    """è°ƒç”¨æ™ºè°±AIæ¨¡å‹"""
    try:
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä½ ç°æœ‰çš„æ™ºè°±AIè°ƒç”¨ä»£ç 
        return f"[æ™ºè°±AI {model_name}] è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›å¤ï¼š{prompt[:50]}..."
    except Exception as e:
        raise Exception(f"æ™ºè°±AIæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")

def call_wenxin_model(model_name: str, prompt: str, temperature: float) -> str:
    """è°ƒç”¨ç™¾åº¦æ–‡å¿ƒæ¨¡å‹"""
    try:
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨ä½ ç°æœ‰çš„æ–‡å¿ƒæ¨¡å‹è°ƒç”¨ä»£ç 
        return f"[ç™¾åº¦æ–‡å¿ƒ {model_name}] è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿå›å¤ï¼š{prompt[:50]}..."
    except Exception as e:
        raise Exception(f"æ–‡å¿ƒæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")

def generate_mock_response(prompt: str) -> str:
    """ç”Ÿæˆæ¨¡æ‹Ÿå›å¤"""
    return f"è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„AIå›å¤ï¼Œé’ˆå¯¹æ‚¨çš„é—®é¢˜ï¼š{prompt[:100]}..."

def generate_llm_only_answer(query: str, model_name: str = "gpt-3.5-turbo", temperature: float = 0.7) -> str:
    """ä»…ä½¿ç”¨å¤§æ¨¡å‹å›ç­”ï¼Œä¸ä¾èµ–çŸ¥è¯†åº“"""
    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»´ä¿®åŠ©æ‰‹ã€‚ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä½ çš„è®­ç»ƒçŸ¥è¯†å›ç­”è¿™ä¸ªç»´ä¿®é—®é¢˜ã€‚å¦‚æœæ˜¯å…³äºè®¾å¤‡ç»´ä¿®çš„é—®é¢˜ï¼Œè¯·æä¾›ï¼š
1. é—®é¢˜å¯èƒ½çš„åŸå› åˆ†æ
2. è¯¦ç»†çš„è§£å†³æ­¥éª¤
3. æ‰€éœ€å·¥å…·å’Œæ³¨æ„äº‹é¡¹
4. é¢„é˜²æªæ–½å»ºè®®

å¦‚æœä¸æ˜¯ç»´ä¿®ç›¸å…³é—®é¢˜ï¼Œè¯·ç¤¼è²Œåœ°å¼•å¯¼ç”¨æˆ·æé—®ç»´ä¿®ç›¸å…³é—®é¢˜ã€‚

å›ç­”è¦ä¸“ä¸šã€è¯¦ç»†ã€å®ç”¨ã€‚"""

    try:
        response = call_llm_model(model_name, prompt, temperature)
        return response
    except Exception as e:
        return f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{str(e)}ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚"

def generate_kb_only_answer(query: str, contexts: List[dict]) -> str:
    """ä»…åŸºäºçŸ¥è¯†åº“å›ç­”ï¼Œä¸ä½¿ç”¨å¤§æ¨¡å‹"""
    if not contexts:
        return "æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚å»ºè®®æ‚¨ï¼š\n1. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®\n2. é€‰æ‹©'å¤§æ¨¡å‹å›ç­”'è·å–AIçš„å»ºè®®\n3. ä¸Šä¼ ç›¸å…³çš„ç»´ä¿®æ–‡æ¡£åˆ°çŸ¥è¯†åº“"
    
    answer_parts = []
    answer_parts.append(f"æ ¹æ®çŸ¥è¯†åº“æ£€ç´¢ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n")
    
    for i, context in enumerate(contexts[:2], 1):
        content = context['content']
        title = context.get('title', 'ç»´ä¿®æŒ‡å—')
        
        answer_parts.append(f"ğŸ“‹ **å‚è€ƒèµ„æ–™ {i}ï¼š{title}**\n")
        
        if isinstance(content, dict):
            # ç»“æ„åŒ–å†…å®¹
            if content.get('steps'):
                answer_parts.append("ğŸ”§ **ç»´ä¿®æ­¥éª¤ï¼š**")
                for j, step in enumerate(content['steps'][:4], 1):
                    answer_parts.append(f"   {j}. {step}")
                answer_parts.append("")
            
            if content.get('tools'):
                tools_str = "ã€".join(content['tools'][:5])
                answer_parts.append(f"ğŸ› ï¸ **æ‰€éœ€å·¥å…·ï¼š** {tools_str}\n")
            
            if content.get('warnings'):
                answer_parts.append("âš ï¸ **æ³¨æ„äº‹é¡¹ï¼š**")
                for warning in content['warnings'][:2]:
                    answer_parts.append(f"   â€¢ {warning}")
                answer_parts.append("")
        else:
            # çº¯æ–‡æœ¬å†…å®¹
            lines = content.split('\n')[:8]
            for line in lines:
                if line.strip():
                    answer_parts.append(f"   {line.strip()}")
            answer_parts.append("")
    
    answer_parts.append("ğŸ’¡ **æç¤ºï¼š** ä»¥ä¸Šä¿¡æ¯æ¥è‡ªçŸ¥è¯†åº“æ–‡æ¡£ï¼Œå»ºè®®ç»“åˆå®é™…æƒ…å†µæ“ä½œã€‚")
    
    return "\n".join(answer_parts)

def generate_auto_answer(query: str, contexts: List[dict], model_name: str = "gpt-3.5-turbo", temperature: float = 0.7) -> str:
    """æ™ºèƒ½é€‰æ‹©å›ç­”æ¨¡å¼"""
    if not contexts:
        # æ²¡æœ‰ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹
        return generate_llm_only_answer(query, model_name, temperature)
    else:
        # æœ‰çŸ¥è¯†åº“å†…å®¹ï¼Œç»“åˆçŸ¥è¯†åº“å’Œå¤§æ¨¡å‹
        return generate_enhanced_answer(query, contexts, model_name, temperature)

@app.post("/api/v1/qa", response_model=QAResponse)
async def get_answer_v2(request: QARequest):
    """å¢å¼ºç‰ˆé—®ç­”æ¥å£ï¼Œæ”¯æŒå¤šç§å›ç­”æ¨¡å¼"""
    import time
    start_time = time.time()
    
    try:
        answer = ""
        sources = []
        confidence = 0.5
        contexts = []
        
        # æ ¹æ®å›ç­”æ¨¡å¼å¤„ç†
        if request.answer_mode == "llm_only":
            # ä»…ä½¿ç”¨å¤§æ¨¡å‹
            answer = generate_llm_only_answer(request.query, request.model, request.temperature)
            confidence = 0.8
            
        elif request.answer_mode == "kb_only":
            # ä»…ä½¿ç”¨çŸ¥è¯†åº“
            contexts = enhanced_search(request.query, request.context_size)
            answer = generate_kb_only_answer(request.query, contexts)
            sources = [ctx.get('url', 'å†…éƒ¨çŸ¥è¯†åº“') for ctx in contexts]
            confidence = 0.9 if contexts else 0.3
            
        else:  # auto
            # æ™ºèƒ½é€‰æ‹©æ¨¡å¼
            contexts = enhanced_search(request.query, request.context_size)
            answer = generate_auto_answer(request.query, contexts, request.model, request.temperature)
            sources = [ctx.get('url', 'å†…éƒ¨çŸ¥è¯†åº“') for ctx in contexts]
            confidence = 0.9 if contexts else 0.8
        
        # ç”Ÿæˆç›¸å…³é—®é¢˜
        related_questions = generate_related_questions(request.query, contexts)
        
        # ä¿å­˜å¯¹è¯å†å²
        from datetime import datetime
        chat_history.append({
            'question': request.query,
            'answer': answer,
            'model': request.model,
            'answer_mode': request.answer_mode,
            'timestamp': datetime.now().isoformat()
        })
        
        processing_time = time.time() - start_time
        
        return QAResponse(
            answer=answer,
            sources=sources,
            confidence=confidence,
            related_questions=related_questions,
            model_used=request.model,
            answer_mode=request.answer_mode,
            processing_time=round(processing_time, 2)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é—®ç­”å¤„ç†å¤±è´¥: {str(e)}")

# ä¿æŒå…¼å®¹æ€§çš„æ—§æ¥å£
@app.get("/api/v1/qa")
async def get_answer_legacy(
    query: str = Query(..., description="ç”¨æˆ·çš„é—®é¢˜"),
    context_size: int = Query(3, description="è¿”å›çš„ç›¸å…³ä¸Šä¸‹æ–‡æ•°é‡"),
    model: str = Query("gpt-3.5-turbo", description="ä½¿ç”¨çš„æ¨¡å‹"),
    temperature: float = Query(0.7, description="æ¨¡å‹æ¸©åº¦å‚æ•°"),
    answer_mode: str = Query("auto", description="å›ç­”æ¨¡å¼")
):
    """å…¼å®¹æ—§ç‰ˆæœ¬çš„GETæ¥å£"""
    request = QARequest(
        query=query,
        answer_mode=answer_mode,
        model=model,
        temperature=temperature,
        context_size=context_size
    )
    return await get_answer_v2(request)

# å¯åŠ¨æ—¶åŠ è½½çŸ¥è¯†åº“
@app.on_event("startup")
async def startup_event():
    load_knowledge_base()

# APIç«¯ç‚¹
@app.get("/api/v1/models")
async def get_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        models = get_available_models()
        return {
            "models": models,
            "default": "gpt-3.5-turbo"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/qa")
async def get_answer(
    query: str = Query(..., description="ç”¨æˆ·çš„é—®é¢˜"),
    context_size: int = Query(3, description="è¿”å›çš„ç›¸å…³ä¸Šä¸‹æ–‡æ•°é‡"),
    model: str = Query("gpt-3.5-turbo", description="ä½¿ç”¨çš„æ¨¡å‹"),
    temperature: float = Query(0.7, description="æ¨¡å‹æ¸©åº¦å‚æ•°")
):
    try:
        # æœç´¢ç›¸å…³å†…å®¹
        contexts = enhanced_search(query, context_size)
        
        # ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç”Ÿæˆå›ç­”
        answer = generate_enhanced_answer(query, contexts, model, temperature)
        
        # ç”Ÿæˆç›¸å…³é—®é¢˜
        related_questions = generate_related_questions(query, contexts)
        
        # ä¿å­˜å¯¹è¯å†å²
        from datetime import datetime
        chat_history.append({
            'question': query,
            'answer': answer,
            'model': model,
            'timestamp': datetime.now().isoformat()
        })
        
        return QAResponse(
            answer=answer,
            sources=[ctx.get('url', 'å†…éƒ¨çŸ¥è¯†åº“') for ctx in contexts],
            confidence=0.9 if contexts else 0.3,
            related_questions=related_questions,
            model_used=model
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        # åˆ›å»ºä¸Šä¼ ç›®å½• - ä¿®å¤è·¯å¾„
        upload_dir = "data/uploads"
        os.makedirs(upload_dir, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        file_id = str(uuid.uuid4())
        file_ext = os.path.splitext(file.filename)[1]
        save_path = os.path.join(upload_dir, f"{file_id}{file_ext}")
        
        contents = await file.read()
        with open(save_path, 'wb') as f:
            f.write(contents)
        
        # å¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼Œå°è¯•å¤„ç†å¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“
        if file_ext.lower() in ['.txt', '.md']:
            try:
                text_content = contents.decode('utf-8')
                structured_content = structure_repair_content(text_content)
                knowledge_base.append({
                    'content': structured_content,
                    'title': file.filename,
                    'url': save_path,
                    'type': 'user_upload',
                    'keywords': extract_keywords(text_content)
                })
                print(f"âœ… å·²å°†ä¸Šä¼ æ–‡ä»¶æ·»åŠ åˆ°çŸ¥è¯†åº“: {file.filename}")
            except:
                pass
        
        return UploadResponse(
            success=True,
            message=f"æ–‡ä»¶ {file.filename} ä¸Šä¼ æˆåŠŸå¹¶å·²æ·»åŠ åˆ°çŸ¥è¯†åº“",
            file_id=file_id
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/collect")
async def collect_data(request: CollectRequest):
    try:
        # æ¨¡æ‹Ÿæ•°æ®é‡‡é›†ï¼ˆå®é™…é¡¹ç›®ä¸­å¯ä»¥å®ç°ç½‘é¡µçˆ¬è™«ï¼‰
        return {
            "success": True,
            "message": f"å¼€å§‹é‡‡é›†ç½‘å€: {request.url}",
            "status": "processing"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/chat/history")
async def get_chat_history(limit: int = Query(10)):
    """è·å–å¯¹è¯å†å²"""
    return {"history": chat_history[-limit:]}

@app.delete("/api/v1/chat/history")
async def clear_chat_history():
    """æ¸…ç©ºå¯¹è¯å†å²"""
    global chat_history
    chat_history = []
    return {"message": "å¯¹è¯å†å²å·²æ¸…ç©º"}

@app.get("/api/v1/knowledge/stats")
async def get_knowledge_stats():
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = {
            "total_documents": len(knowledge_base),
            "types": {},
            "total_steps": 0,
            "total_tools": set(),
            "total_warnings": 0
        }
        
        for item in knowledge_base:
            doc_type = item.get('type', 'unknown')
            stats['types'][doc_type] = stats['types'].get(doc_type, 0) + 1
            
            if isinstance(item['content'], dict):
                stats['total_steps'] += len(item['content'].get('steps', []))
                stats['total_tools'].update(item['content'].get('tools', []))
                stats['total_warnings'] += len(item['content'].get('warnings', []))
        
        stats['total_tools'] = len(stats['total_tools'])
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/knowledge/recent")
async def get_recent_activity(limit: int = Query(10)):
    """è·å–æœ€è¿‘çš„çŸ¥è¯†åº“æ´»åŠ¨"""
    try:
        # æ¨¡æ‹Ÿæœ€è¿‘æ´»åŠ¨æ•°æ®
        recent_activities = []
        
        # ä»knowledge_baseè·å–æœ€è¿‘æ·»åŠ çš„æ–‡æ¡£
        for i, item in enumerate(knowledge_base[-limit:]):
            activity = {
                "type": "upload" if "user_upload" in item.get('type', '') else "collect",
                "title": item.get('title', f'æ–‡æ¡£ {i+1}')[:50],
                "time": "åˆšåˆš" if i < 3 else f"{i*2}åˆ†é’Ÿå‰",
                "status": "success",
                "url": item.get('url', ''),
                "doc_type": item.get('type', 'unknown')
            }
            recent_activities.append(activity)
        
        return {"activities": recent_activities}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/crawl")
async def start_crawl(request: CollectRequest):
    """å¯åŠ¨çˆ¬è™«ä»»åŠ¡ - å¢å¼ºç‰ˆ"""
    try:
        # éªŒè¯URL
        if not request.url.startswith(('http://', 'https://')):
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„URLæ ¼å¼")
        
        # æ¨¡æ‹Ÿçˆ¬è™«å¤„ç†
        task_id = f"task_{len(knowledge_base) + 1}_{int(time.time())}"
        
        # è¿™é‡Œå¯ä»¥é›†æˆçœŸå®çš„çˆ¬è™«é€»è¾‘
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿå“åº”
        response = {
            "success": True,
            "message": f"æˆåŠŸå¯åŠ¨é‡‡é›†ä»»åŠ¡",
            "task_id": task_id,
            "status": "started",
            "url": request.url,
            "estimated_time": "é¢„è®¡2-5åˆ†é’Ÿå®Œæˆ"
        }
        
        # æ¨¡æ‹Ÿæ·»åŠ åˆ°çŸ¥è¯†åº“ï¼ˆå®é™…åº”è¯¥åœ¨çˆ¬è™«å®Œæˆåï¼‰
        simulated_doc = {
            'content': {
                'summary': f"ä» {request.url} é‡‡é›†çš„ç»´ä¿®æŒ‡å—",
                'steps': [f"æ­¥éª¤1: ä»{request.url}é‡‡é›†åˆ°çš„å†…å®¹"],
                'tools': ['çˆ¬è™«å·¥å…·'],
                'warnings': ['è¯·éªŒè¯é‡‡é›†å†…å®¹çš„å‡†ç¡®æ€§']
            },
            'title': f"é‡‡é›†è‡ª {request.url}",
            'url': request.url,
            'type': 'web_crawl',
            'keywords': ['é‡‡é›†', 'åœ¨çº¿èµ„æº']
        }
        knowledge_base.append(simulated_doc)
        
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
async def root():
    return {
        "message": "å®¶å…·ç»´ä¿®æ™ºèƒ½åŠ©æ‰‹API",
        "status": "running",
        "version": "1.0.0",
        "description": "åŸºäºRAGæŠ€æœ¯çš„æ™ºèƒ½ç»´ä¿®é—®ç­”ç³»ç»Ÿ"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
