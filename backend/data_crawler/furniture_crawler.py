import requests
from bs4 import BeautifulSoup
import re
import os
import json
import urllib3
from .utils import clean_text

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class FurnitureCrawler:
    def __init__(self, start_urls=None, base_domain="https://zh.ifixit.com"):
        self.start_urls = start_urls or ["/Device/Huawei_P"]
        self.base_domain = base_domain
        self.visited_pages = set()
        self.saved_links = set()
        
        self.valid_crawl_prefixes = ["/Device/", "/Guide/", "/Wiki/"]
        self.valid_save_prefixes = ["/Guide/", "/Wiki/"]
        self.excluded_keywords = [
            "/edit/", "/translate/", "/history/",
            "/Edit/", "/Translate/", "/History/"
        ]
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/114.0.0.0 Safari/537.36"
        }

    def should_visit(self, href):
        return (
            href.startswith("/") and
            any(href.startswith(prefix) for prefix in self.valid_crawl_prefixes) and
            not any(excluded in href for excluded in self.excluded_keywords)
        )

    def is_valid_save_url(self, href):
        return (
            any(href.startswith(prefix) for prefix in self.valid_save_prefixes) and
            not any(x in href for x in self.excluded_keywords) and
            re.search(r"/\d+$", href)
        )

    def crawl_page(self, path):
        if any(excluded in path for excluded in self.excluded_keywords):
            print(f"â›” è·³è¿‡ç¦æ­¢è®¿é—®é¡µé¢: {path}")
            return

        full_url = self.base_domain + path
        if full_url in self.visited_pages:
            return
        print(f"ğŸ“„ æ­£åœ¨è®¿é—®: {full_url}")
        self.visited_pages.add(full_url)

        try:
            response = requests.get(full_url, headers=self.headers, timeout=10, verify=False)
            response.raise_for_status()
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥: {full_url} åŸå› : {e}")
            return

        soup = BeautifulSoup(response.text, "html.parser")
        for a_tag in soup.find_all("a", href=True):
            href = a_tag['href'].split("#")[0].strip()
            if not self.should_visit(href):
                continue

            if self.is_valid_save_url(href):
                url = self.base_domain + href
                if "lang=" not in url:
                    url += "?lang=en"
                self.saved_links.add(url)
            else:
                self.crawl_page(href)

    def fetch_clean_text_and_title(self, url: str):
        try:
            response = requests.get(url, headers=self.headers, timeout=10, verify=False)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            title = soup.title.string.strip() if soup.title and soup.title.string else ""
            main_content = soup.find("article") or soup.find("main") or soup.body

            for tag in main_content(['script', 'style', 'noscript', 'footer', 'nav', 'aside']):
                tag.decompose()

            raw_text = main_content.get_text(separator="\n", strip=True)
            lines = [line.strip() for line in raw_text.splitlines() if line.strip()]

            # ä½¿ç”¨ utils ä¸­çš„æ¸…æ´—åŠŸèƒ½
            clean_lines = [clean_text(line) for line in lines if not self._is_noise(line)]
            clean_text_content = "\n".join(clean_lines)

            return {"url": url, "title": title, "content": clean_text_content}

        except Exception as e:
            print(f"[ERROR] æŠ“å–å¤±è´¥ï¼š{url}  åŸå› ï¼š{e}")
            return None

    def _is_noise(self, line: str) -> bool:
        exact_phrases = [
            "ç”±è¡·æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼š", "ä¿®å¤ä½ çš„ç‰©å“", "è·³è½¬åˆ°ä¸»å†…å®¹", "ç¤¾åŒº", "å•†åº—", "ç¿»è¯‘",
            "å›å¤", "æ·»åŠ è¯„è®º", "å–æ¶ˆ", "å‘å¸–è¯„è®º"
        ]

        partial_keywords = [
            "ä½œè€…", "ä¸", "çš„ä¼šå‘˜", "å›¢é˜Ÿ", "å¾½ç« ", "åˆ›ä½œäº†", "å£°æœ›", "æ³¨å†Œ", "åæˆå‘˜",
            "Author", "Registered on", "reputation", "Created", "guides",
            "Badges", "more badges", "Team", "member of", "Community", "members",
            "Thanks", "thank you", "Grazie", "grandissimo lavoro", "special thanks",
            "Harry Mao", "Ennis", "Alisha C", "Vincenzo Garletti", "Tozyel Lagaffe"
        ]

        regex_patterns = [
            re.compile(r"æµè§ˆç»Ÿè®¡æ•°æ®[:ï¼š]?$"),
            re.compile(r"è¿‡å»\s*(24\s*å°æ—¶|7\s*å¤©|30\s*å¤©)[ï¼š:]?$"),
            re.compile(r"^æ€»è®¡[ï¼š:]?$"),
            re.compile(r"(è¿‡å»\s?\d+\s?[å¤©å°æ—¶]+|æ€»è®¡)\s?[ï¼š:]\s?[\d,]+"),
            re.compile(r"^\s*[\d,]+\s*$"),
            re.compile(r"^\s*\d+%?\s*$"),
            re.compile(r"^\s*(en|zh)\s*$", re.IGNORECASE),
            re.compile(r"^\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}"),
            re.compile(r"^\w+\s+\w+\s*-\s*\d{4}"),
            re.compile(r"äº\d{1,2}/\d{1,2}/\d{2,4}æ³¨å†Œ"),
            re.compile(r"\d{1,3}(,\d{3})*\s*å£°æœ›"),
            re.compile(r"\d{1,3}(,\d{3})*\s*reputation", re.I),
            re.compile(r"åˆ›ä½œäº†\d+\s*ç¯‡æŒ‡å—"),
            re.compile(r"Created\s+\d+\s+guides", re.I),
            re.compile(r"\+?\s*\d+\s*æ›´å¤šå¾½ç« "),
            re.compile(r"Badges[:ï¼š]?\s*\+?\d+\s+more", re.I),
            re.compile(r"\d+\s*åæˆå‘˜"),
            re.compile(r"\d+\s*members", re.I),
        ]

        if line in exact_phrases:
            return True
        if any(keyword in line for keyword in partial_keywords):
            return True
        if any(pat.search(line) for pat in regex_patterns):
            return True
        return False

    def crawl(self, output_json="clean_results_cleaned.json", output_urls="urls.txt"):
        # å¼€å§‹çˆ¬å–
        for start_path in self.start_urls:
            self.crawl_page(start_path)

        # ä¿å­˜é“¾æ¥
        new_links = sorted(self.saved_links)
        os.makedirs(os.path.dirname(output_urls) if os.path.dirname(output_urls) else '.', exist_ok=True)
        
        with open(output_urls, "w", encoding="utf-8") as f:
            for url in new_links:
                f.write(url + "\n")

        print(f"\nâœ… å…±ä¿å­˜ {len(new_links)} æ¡å¯æŠ“å–å†…å®¹é“¾æ¥åˆ°ï¼š{output_urls}")

        # æŠ“å–å†…å®¹å¹¶æ¸…æ´—
        results = []
        for idx, url in enumerate(new_links, 1):
            print(f"\n[{idx}/{len(new_links)}] æ­£åœ¨æŠ“å–å¹¶æ¸…æ´—ï¼š{url}")
            item = self.fetch_clean_text_and_title(url)
            if item and item["content"]:
                results.append(item)
                print(f"âœ”ï¸ æŠ“å–æˆåŠŸï¼š{len(item['content'])} å­—ç¬¦")
            else:
                print("âš ï¸ è·³è¿‡ç©ºé¡µé¢æˆ–å¤±è´¥é¡µé¢")

        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_json) if os.path.dirname(output_json) else '.', exist_ok=True)
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ‰ æ‰€æœ‰å¤„ç†ç»“æœå·²ä¿å­˜ä¸ºï¼š{output_json}")
        return results