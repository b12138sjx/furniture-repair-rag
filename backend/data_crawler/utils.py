import re
import os
from typing import List, Dict, Optional


def clean_text(text):
    """å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦"""
    cleaned = re.sub(r'\s+', ' ', text).strip()
    return cleaned


def should_remove_block(text_block: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç§»é™¤æ–‡æœ¬å—ï¼ˆUIå…ƒç´ ç­‰å™ªéŸ³ï¼‰"""
    ui_keywords = ["ç¼–è¾‘", "æ·»åŠ ä¸€æ¡è¯„è®º", "ç¼–è¾‘æŒ‡å—", "æ˜¾ç¤ºæ›´å¤š", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ", "å°â€”â€”", "ä¸­â€”â€”", "å¤§â€”â€”"]
    pixel_pattern = re.compile(r"[å°ä¸­å¤§][â€”\-â€“â€”]{1,2}\s*\d{2,4}\s*åƒç´ ")
    return any(kw in text_block for kw in ui_keywords) or pixel_pattern.search(text_block)


def extract_main_content(soup):
    """ä»BeautifulSoupå¯¹è±¡ä¸­æå–ä¸»è¦å†…å®¹"""
    main_content = soup.find("article") or soup.find("main") or soup.body
    
    # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
    for tag in main_content(['script', 'style', 'noscript', 'footer', 'nav', 'aside']):
        tag.decompose()
    
    return main_content


def normalize_url(url, base_domain):
    """æ ‡å‡†åŒ–URLæ ¼å¼"""
    if not url.startswith("http"):
        url = base_domain + url
    if "lang=" not in url:
        url += "?lang=en"
    return url


def is_valid_furniture_content(text):
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„å®¶å…·ç»´ä¿®å†…å®¹"""
    # æ‰©å±•å…³é”®è¯ä»¥åŒ…å«æ›´å¤šç»´ä¿®ç›¸å…³æœ¯è¯­
    furniture_keywords = [
        "ç»´ä¿®", "ä¿®ç†", "ä¿å…»", "å®¶å…·", "æ²™å‘", "æ¡Œå­", "æ¤…å­", "åºŠ", "æŸœå­",
        "repair", "fix", "maintenance", "furniture", "sofa", "table", "chair", "bed", "cabinet",
        "replacement", "battery", "screen", "camera", "speaker", "antenna", "engine",
        "æ‹†è§£", "å®‰è£…", "æ›´æ¢", "æ­¥éª¤", "å·¥å…·", "èºä¸", "adhesive", "assembly"
    ]
    text_lower = text.lower()
    return any(keyword.lower() in text_lower for keyword in furniture_keywords)


def extract_repair_steps(content: str) -> List[str]:
    """æå–ç»´ä¿®æ­¥éª¤"""
    step_pattern = re.compile(r'æ­¥éª¤\s*\d+|Step\s*\d+|^\d+\.', re.MULTILINE)
    steps = []
    
    lines = content.split('\n')
    current_step = []
    
    for line in lines:
        line = line.strip()
        if step_pattern.match(line):
            if current_step:
                steps.append('\n'.join(current_step))
                current_step = []
            current_step.append(line)
        elif current_step and line:
            current_step.append(line)
    
    if current_step:
        steps.append('\n'.join(current_step))
    
    return steps


def get_file_stats(file_path: str) -> Dict:
    """è·å–æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    if not os.path.exists(file_path):
        return {}
    
    stat = os.stat(file_path)
    return {
        'size_bytes': stat.st_size,
        'size_mb': round(stat.st_size / 1024 / 1024, 2),
        'modified_time': stat.st_mtime
    }


def create_summary_report(data: Dict) -> str:
    """åˆ›å»ºæ•°æ®æ‘˜è¦æŠ¥å‘Š"""
    report = []
    report.append("ğŸ“‹ æ•°æ®æ•´åˆæ‘˜è¦æŠ¥å‘Š")
    report.append("=" * 40)
    
    if 'stats' in data:
        stats = data['stats']
        report.append(f"ğŸ“„ æ–‡æ¡£æ€»æ•°: {stats.get('total_documents', 0)}")
        report.append(f"ğŸ”— URLæ€»æ•°: {stats.get('total_urls', 0)}")
        if 'json_documents' in stats:
            report.append(f"ğŸ“± JSONæ–‡æ¡£æ•°: {stats['json_documents']}")
    
    if 'documents' in data:
        docs = data['documents']
        if docs:
            avg_length = sum(doc['metadata'].get('length', 0) for doc in docs) / len(docs)
            report.append(f"ğŸ“ å¹³å‡æ–‡æ¡£é•¿åº¦: {int(avg_length)} å­—ç¬¦")
            
            types = {}
            for doc in docs:
                doc_type = doc['metadata'].get('type', 'unknown')
                types[doc_type] = types.get(doc_type, 0) + 1
            
            report.append("ğŸ“Š æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
            for doc_type, count in types.items():
                report.append(f"   - {doc_type}: {count}")
    
    return '\n'.join(report)