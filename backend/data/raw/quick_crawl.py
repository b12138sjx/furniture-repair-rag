#!/usr/bin/env python3
"""
å¿«é€ŸæŠ“å–è„šæœ¬ - ç›´æ¥ä½¿ç”¨å·²æœ‰çš„URLåˆ—è¡¨
"""
import json
import os
import sys
import requests
from bs4 import BeautifulSoup
import re
import time
from typing import List, Dict, Optional

# ä¿®å¤å¯¼å…¥é—®é¢˜ - ç›´æ¥åœ¨æ–‡ä»¶ä¸­å®šä¹‰å‡½æ•°

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/114.0.0.0 Safari/537.36"
}

def load_urls_from_file(file_path: str = "phone urls.txt") -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½å·²æœ‰çš„URLåˆ—è¡¨"""
    urls = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and url.startswith('http'):
                    urls.append(url)
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½äº† {len(urls)} ä¸ªURL")
    return urls

def clean_text_lines(lines: List[str]) -> List[str]:
    """æ¸…ç†æ–‡æœ¬è¡Œï¼Œç§»é™¤å™ªéŸ³"""
    
    # è¦å®Œå…¨ç§»é™¤çš„çŸ­è¯­
    exact_phrases = [
        "ç”±è¡·æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼š", "ä¿®å¤ä½ çš„ç‰©å“", "è·³è½¬åˆ°ä¸»å†…å®¹", "ç¤¾åŒº", "å•†åº—", "ç¿»è¯‘",
        "å›å¤", "æ·»åŠ è¯„è®º", "å–æ¶ˆ", "å‘å¸–è¯„è®º", "ç¼–è¾‘", "å†å²", "å·¥å…·", "é›¶ä»¶",
        "éš¾åº¦", "æ­¥éª¤", "æ—¶é—´è¦æ±‚"
    ]

    # éƒ¨åˆ†å…³é”®è¯
    partial_keywords = [
        "ä½œè€…", "ä¸", "çš„ä¼šå‘˜", "å›¢é˜Ÿ", "å¾½ç« ", "åˆ›ä½œäº†", "å£°æœ›", "æ³¨å†Œ", "åæˆå‘˜",
        "Author", "Registered on", "reputation", "Created", "guides",
        "Badges", "more badges", "Team", "member of", "Community", "members"
    ]

    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    regex_patterns = [
        re.compile(r"æµè§ˆç»Ÿè®¡æ•°æ®[:ï¼š]?$"),
        re.compile(r"è¿‡å»\s*(24\s*å°æ—¶|7\s*å¤©|30\s*å¤©)[ï¼š:]?$"),
        re.compile(r"^æ€»è®¡[ï¼š:]?$"),
        re.compile(r"^\s*[\d,]+\s*$"),
        re.compile(r"^\s*\d+%?\s*$"),
        re.compile(r"^\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}"),
        re.compile(r"\d{1,3}(,\d{3})*\s*å£°æœ›"),
    ]

    def is_noise(line: str) -> bool:
        # ...existing code for noise detection...
        line_lower = line.lower()
        
        if line in exact_phrases:
            return True
            
        if any(keyword.lower() in line_lower for keyword in partial_keywords):
            return True
            
        if any(pattern.search(line) for pattern in regex_patterns):
            return True
            
        if len(line) < 3:
            return True
            
        return False

    return [line for line in lines if not is_noise(line)]

def fetch_clean_text_and_title(url: str) -> Optional[Dict]:
    """æŠ“å–å¹¶æ¸…æ´—å•ä¸ªURLçš„å†…å®¹"""
    try:
        print(f"ğŸ”„ æ­£åœ¨æŠ“å–: {url}")
        response = requests.get(url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        # æå–æ ‡é¢˜
        title = ""
        title_selectors = ["h1.pagetitle", "h1", "title"]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                break

        # æå–ä¸»è¦å†…å®¹
        main_content = None
        content_selectors = [
            "article", ".guide-content", ".wiki-content", "main", "#main-content", ".content"
        ]
        
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        if not main_content:
            main_content = soup.body

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        unwanted_selectors = [
            'script', 'style', 'noscript', 'footer', 'nav', 'aside',
            '.advertisement', '.ads', '.social-share', '.comments',
            '.breadcrumb', '.sidebar', '.header', '.navigation'
        ]
        
        for selector in unwanted_selectors:
            for element in main_content.select(selector):
                element.decompose()

        # æå–æ–‡æœ¬
        raw_text = main_content.get_text(separator="\n", strip=True)
        lines = [line.strip() for line in raw_text.splitlines() if line.strip()]

        # æ¸…ç†å™ªéŸ³æ–‡æœ¬
        clean_lines = clean_text_lines(lines)
        clean_text = "\n".join(clean_lines)

        # éªŒè¯å†…å®¹è´¨é‡
        if len(clean_text) < 100:
            print(f"âš ï¸  å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡: {url}")
            return None

        result = {
            "url": url,
            "title": title,
            "content": clean_text
        }
        
        print(f"âœ… æˆåŠŸæŠ“å–: {title} ({len(clean_text)} å­—ç¬¦)")
        return result

    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {url} - {e}")
        return None

def batch_crawl_from_urls(url_file: str = "phone urls.txt", 
                         output_file: str = "phone.json",
                         start_index: int = 0,
                         max_urls: int = 50) -> List[Dict]:
    """æ‰¹é‡æŠ“å–URLåˆ—è¡¨ä¸­çš„å†…å®¹"""
    
    # åŠ è½½URLåˆ—è¡¨
    urls = load_urls_from_file(url_file)
    if not urls:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„URL")
        return []
    
    # é™åˆ¶æŠ“å–æ•°é‡
    urls_to_crawl = urls[start_index:start_index + max_urls]
    print(f"ğŸ“‹ å‡†å¤‡æŠ“å– {len(urls_to_crawl)} ä¸ªURL (ç´¢å¼• {start_index} åˆ° {start_index + len(urls_to_crawl)})")
    
    results = []
    success_count = 0
    
    for i, url in enumerate(urls_to_crawl, 1):
        print(f"\n[{i}/{len(urls_to_crawl)}] å¤„ç†URL...")
        
        # æŠ“å–å†…å®¹
        item = fetch_clean_text_and_title(url)
        if item and item["content"]:
            if len(item["content"]) > 200:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                results.append(item)
                success_count += 1
                print(f"âœ… æˆåŠŸå¤„ç†ç¬¬ {success_count} æ¡æ•°æ®")
            else:
                print(f"âš ï¸  å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥æˆ–å†…å®¹ä¸ºç©º")
        
        # æ·»åŠ å»¶æ—¶é¿å…è¢«å°
        if i % 5 == 0:  # æ¯5ä¸ªè¯·æ±‚ä¼‘æ¯ä¸€ä¸‹
            print("ğŸ˜´ ä¼‘æ¯ 2 ç§’...")
            time.sleep(2)
        else:
            time.sleep(0.5)
    
    # ä¿å­˜ç»“æœ
    if results:
        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except:
                existing_data = []
        
        # åˆå¹¶æ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰
        existing_urls = {item.get('url', '') for item in existing_data}
        new_results = [item for item in results if item['url'] not in existing_urls]
        
        if new_results:
            all_data = existing_data + new_results
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ‰ æˆåŠŸä¿å­˜ {len(new_results)} æ¡æ–°æ•°æ®åˆ° {output_file}")
            print(f"ğŸ“Š æ–‡ä»¶æ€»è®¡åŒ…å« {len(all_data)} æ¡æ•°æ®")
        else:
            print(f"\nâš ï¸  æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
    else:
        print(f"\nâŒ æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•æ•°æ®")
    
    return results

def main():
    print("ğŸš€ å¿«é€Ÿæ•°æ®æŠ“å–å·¥å…·")
    print("=" * 40)
    
    # æ£€æŸ¥URLæ–‡ä»¶
    url_file = "phone urls.txt"
    if not os.path.exists(url_file):
        print(f"âŒ æ‰¾ä¸åˆ°URLæ–‡ä»¶: {url_file}")
        return
    
    # ç»Ÿè®¡URLæ•°é‡
    with open(url_file, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(urls)} ä¸ªURL")
    
    # è®¾ç½®æŠ“å–å‚æ•°
    start_index = int(input(f"èµ·å§‹ç´¢å¼• (0-{len(urls)-1}, é»˜è®¤0): ") or "0")
    max_urls = int(input("æŠ“å–æ•°é‡ (é»˜è®¤30): ") or "30")
    
    print(f"\nğŸ”„ å¼€å§‹æŠ“å–...")
    print(f"ğŸ“ èµ·å§‹ä½ç½®: {start_index}")
    print(f"ğŸ“Š æŠ“å–æ•°é‡: {max_urls}")
    print("=" * 40)
    
    # æ‰§è¡ŒæŠ“å–
    results = batch_crawl_from_urls(
        url_file=url_file,
        output_file="phone.json", 
        start_index=start_index,
        max_urls=max_urls
    )
    
    print("=" * 40)
    print(f"âœ… æŠ“å–å®Œæˆ! æˆåŠŸè·å– {len(results)} æ¡æ•°æ®")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    if os.path.exists("phone.json"):
        with open("phone.json", 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶æ€»è®¡: {len(all_data)} æ¡è®°å½•")

if __name__ == "__main__":
    main()
