import requests
from bs4 import BeautifulSoup
import re
import os
import json
import urllib3
import time
from typing import List, Dict, Optional

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_DOMAIN = "https://zh.ifixit.com"
START_PATHS = ["/Device/Huawei_P"]

visited_pages = set()
saved_links = set()

VALID_CRAWL_PREFIXES = ["/Device/", "/Guide/", "/Wiki/"]
VALID_SAVE_PREFIXES = ["/Guide/", "/Wiki/"]
EXCLUDED_KEYWORDS = [
    "/edit/", "/translate/", "/history/",
    "/Edit/", "/Translate/", "/History/"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/114.0.0.0 Safari/537.36"
}

def should_visit(href):
    return (
        href.startswith("/") and
        any(href.startswith(prefix) for prefix in VALID_CRAWL_PREFIXES) and
        not any(excluded in href for excluded in EXCLUDED_KEYWORDS)
    )

def is_valid_save_url(href):
    return (
        any(href.startswith(prefix) for prefix in VALID_SAVE_PREFIXES) and
        not any(x in href for x in EXCLUDED_KEYWORDS) and
        re.search(r"/\d+$", href)
    )

def crawl_page(path):
    if any(excluded in path for excluded in EXCLUDED_KEYWORDS):
        print(f"â›” è·³è¿‡ç¦æ­¢è®¿é—®é¡µé¢: {path}")
        return

    full_url = BASE_DOMAIN + path
    if full_url in visited_pages:
        return
    print(f"ğŸ“„ æ­£åœ¨è®¿é—®: {full_url}")
    visited_pages.add(full_url)

    try:
        response = requests.get(full_url, headers=HEADERS, timeout=10, verify=False)
        response.raise_for_status()
    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {full_url} åŸå› : {e}")
        return

    soup = BeautifulSoup(response.text, "html.parser")
    for a_tag in soup.find_all("a", href=True):
        href = a_tag['href'].split("#")[0].strip()
        if not should_visit(href):
            continue

        if is_valid_save_url(href):
            url = BASE_DOMAIN + href
            if "lang=" not in url:
                url += "?lang=zh"
            saved_links.add(url)
        else:
            crawl_page(href)

def load_urls_from_file(file_path: str = "phone urls.txt") -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½å·²æœ‰çš„URLåˆ—è¡¨"""
    urls = []
    if os.path.exists(file_path):
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                url = line.strip()
                if url and url.startswith('http'):
                    urls.append(url)
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½äº† {len(urls)} ä¸ªURL")
    return urls

def fetch_clean_text_and_title(url: str) -> Optional[Dict]:
    """æŠ“å–å¹¶æ¸…æ´—å•ä¸ªURLçš„å†…å®¹"""
    try:
        print(f"ğŸ”„ æ­£åœ¨æŠ“å–: {url}")
        response = requests.get(url, headers=HEADERS, timeout=15, verify=False)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        # æå–æ ‡é¢˜
        title = ""
        title_selectors = [
            "h1.pagetitle",
            "h1",
            "title"
        ]
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                break

        # æå–ä¸»è¦å†…å®¹
        main_content = None
        content_selectors = [
            "article",
            ".guide-content", 
            ".wiki-content",
            "main",
            "#main-content",
            ".content"
        ]
        
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        if not main_content:
            main_content = soup.body

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        unwanted_selectors = [
            'script', 'style', 'noscript', 'footer', 'nav', 'aside',
            '.advertisement', '.ads', '.social-share', '.comments',
            '.breadcrumb', '.sidebar', '.header', '.navigation'
        ]
        
        for selector in unwanted_selectors:
            for element in main_content.select(selector):
                element.decompose()

        # æå–æ–‡æœ¬
        raw_text = main_content.get_text(separator="\n", strip=True)
        lines = [line.strip() for line in raw_text.splitlines() if line.strip()]

        # æ¸…ç†å™ªéŸ³æ–‡æœ¬
        clean_lines = clean_text_lines(lines)
        clean_text = "\n".join(clean_lines)

        # éªŒè¯å†…å®¹è´¨é‡
        if len(clean_text) < 100:
            print(f"âš ï¸  å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡: {url}")
            return None

        result = {
            "url": url,
            "title": title,
            "content": clean_text
        }
        
        print(f"âœ… æˆåŠŸæŠ“å–: {title} ({len(clean_text)} å­—ç¬¦)")
        return result

    except Exception as e:
        print(f"âŒ æŠ“å–å¤±è´¥: {url} - {e}")
        return None

def clean_text_lines(lines: List[str]) -> List[str]:
    """æ¸…ç†æ–‡æœ¬è¡Œï¼Œç§»é™¤å™ªéŸ³"""
    
    # è¦å®Œå…¨ç§»é™¤çš„çŸ­è¯­
    exact_phrases = [
        "ç”±è¡·æ„Ÿè°¢ä»¥ä¸‹è¯‘è€…ï¼š", "ä¿®å¤ä½ çš„ç‰©å“", "è·³è½¬åˆ°ä¸»å†…å®¹", "ç¤¾åŒº", "å•†åº—", "ç¿»è¯‘",
        "å›å¤", "æ·»åŠ è¯„è®º", "å–æ¶ˆ", "å‘å¸–è¯„è®º", "ç¼–è¾‘", "å†å²", "å·¥å…·", "é›¶ä»¶",
        "éš¾åº¦", "æ­¥éª¤", "æ—¶é—´è¦æ±‚", "iPhone", "Android", "Mac", "PC"
    ]

    # éƒ¨åˆ†å…³é”®è¯
    partial_keywords = [
        "ä½œè€…", "ä¸", "çš„ä¼šå‘˜", "å›¢é˜Ÿ", "å¾½ç« ", "åˆ›ä½œäº†", "å£°æœ›", "æ³¨å†Œ", "åæˆå‘˜",
        "Author", "Registered on", "reputation", "Created", "guides",
        "Badges", "more badges", "Team", "member of", "Community", "members",
        "Thanks", "thank you", "Grazie", "grandissimo lavoro", "special thanks"
    ]

    # æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
    regex_patterns = [
        re.compile(r"æµè§ˆç»Ÿè®¡æ•°æ®[:ï¼š]?$"),
        re.compile(r"è¿‡å»\s*(24\s*å°æ—¶|7\s*å¤©|30\s*å¤©)[ï¼š:]?$"),
        re.compile(r"^æ€»è®¡[ï¼š:]?$"),
        re.compile(r"(è¿‡å»\s?\d+\s?[å¤©å°æ—¶]+|æ€»è®¡)\s?[ï¼š:]\s?[\d,]+"),
        re.compile(r"^\s*[\d,]+\s*$"),
        re.compile(r"^\s*\d+%?\s*$"),
        re.compile(r"^\s*(en|zh)\s*$", re.IGNORECASE),
        re.compile(r"^\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}"),
        re.compile(r"^\w+\s+\w+\s*-\s*\d{4}"),
        re.compile(r"äº\d{1,2}/\d{1,2}/\d{2,4}æ³¨å†Œ"),
        re.compile(r"\d{1,3}(,\d{3})*\s*å£°æœ›"),
        re.compile(r"\d{1,3}(,\d{3})*\s*reputation", re.I),
        re.compile(r"åˆ›ä½œäº†\d+\s*ç¯‡æŒ‡å—"),
        re.compile(r"Created\s+\d+\s+guides", re.I),
        re.compile(r"\+?\s*\d+\s*æ›´å¤šå¾½ç« "),
        re.compile(r"Badges[:ï¼š]?\s*\+?\d+\s+more", re.I),
        re.compile(r"\d+\s*åæˆå‘˜"),
        re.compile(r"\d+\s*members", re.I),
    ]

    def is_noise(line: str) -> bool:
        line_lower = line.lower()
        
        # æ£€æŸ¥å®Œå…¨åŒ¹é…
        if line in exact_phrases:
            return True
            
        # æ£€æŸ¥éƒ¨åˆ†å…³é”®è¯
        if any(keyword.lower() in line_lower for keyword in partial_keywords):
            return True
            
        # æ£€æŸ¥æ­£åˆ™è¡¨è¾¾å¼
        if any(pattern.search(line) for pattern in regex_patterns):
            return True
            
        # è¿‡æ»¤è¿‡çŸ­çš„è¡Œ
        if len(line) < 3:
            return True
            
        return False

    return [line for line in lines if not is_noise(line)]

def batch_crawl_from_urls(url_file: str = "phone urls.txt", 
                         output_file: str = "phone.json",
                         start_index: int = 0,
                         max_urls: int = 50) -> List[Dict]:
    """æ‰¹é‡æŠ“å–URLåˆ—è¡¨ä¸­çš„å†…å®¹"""
    
    # åŠ è½½URLåˆ—è¡¨
    urls = load_urls_from_file(url_file)
    if not urls:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„URL")
        return []
    
    # é™åˆ¶æŠ“å–æ•°é‡
    urls_to_crawl = urls[start_index:start_index + max_urls]
    print(f"ğŸ“‹ å‡†å¤‡æŠ“å– {len(urls_to_crawl)} ä¸ªURL (ç´¢å¼• {start_index} åˆ° {start_index + len(urls_to_crawl)})")
    
    results = []
    success_count = 0
    
    for i, url in enumerate(urls_to_crawl, 1):
        print(f"\n[{i}/{len(urls_to_crawl)}] å¤„ç†URL...")
        
        # æŠ“å–å†…å®¹
        item = fetch_clean_text_and_title(url)
        if item and item["content"]:
            # è¿›ä¸€æ­¥è¿‡æ»¤
            filtered_lines = [line for line in item["content"].splitlines() 
                            if not should_remove_block(line)]
            item["content"] = "\n".join(filtered_lines).strip()
            
            if len(item["content"]) > 200:  # ç¡®ä¿å†…å®¹è¶³å¤Ÿé•¿
                results.append(item)
                success_count += 1
                print(f"âœ… æˆåŠŸå¤„ç†ç¬¬ {success_count} æ¡æ•°æ®")
            else:
                print(f"âš ï¸  å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡")
        else:
            print(f"âŒ æŠ“å–å¤±è´¥æˆ–å†…å®¹ä¸ºç©º")
        
        # æ·»åŠ å»¶æ—¶é¿å…è¢«å°
        if i % 5 == 0:  # æ¯5ä¸ªè¯·æ±‚ä¼‘æ¯ä¸€ä¸‹
            print("ğŸ˜´ ä¼‘æ¯ 2 ç§’...")
            time.sleep(2)
        else:
            time.sleep(0.5)
    
    # ä¿å­˜ç»“æœ
    if results:
        # è¯»å–ç°æœ‰æ•°æ®
        existing_data = []
        if os.path.exists(output_file):
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except:
                existing_data = []
        
        # åˆå¹¶æ•°æ®ï¼ˆé¿å…é‡å¤ï¼‰
        existing_urls = {item.get('url', '') for item in existing_data}
        new_results = [item for item in results if item['url'] not in existing_urls]
        
        if new_results:
            all_data = existing_data + new_results
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ‰ æˆåŠŸä¿å­˜ {len(new_results)} æ¡æ–°æ•°æ®åˆ° {output_file}")
            print(f"ğŸ“Š æ–‡ä»¶æ€»è®¡åŒ…å« {len(all_data)} æ¡æ•°æ®")
        else:
            print(f"\nâš ï¸  æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
    else:
        print(f"\nâŒ æ²¡æœ‰æˆåŠŸæŠ“å–åˆ°ä»»ä½•æ•°æ®")
    
    return results

def should_remove_block(text_block: str) -> bool:
    ui_keywords = ["ç¼–è¾‘", "æ·»åŠ ä¸€æ¡è¯„è®º", "ç¼–è¾‘æŒ‡å—", "æ˜¾ç¤ºæ›´å¤š", "ä¸Šä¸€é¡µ", "ä¸‹ä¸€é¡µ", "å°â€”â€”", "ä¸­â€”â€”", "å¤§â€”â€”"]
    pixel_pattern = re.compile(r"[å°ä¸­å¤§][â€”\-â€“â€”]{1,2}\s*\d{2,4}\s*åƒç´ ")
    return any(kw in text_block for kw in ui_keywords) or pixel_pattern.search(text_block)

def run_full_process(output_json="phone.json", output_urls="phone urls.txt"):
    """è¿è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹"""
    print("ğŸš€ å¼€å§‹å®Œæ•´çˆ¬å–æµç¨‹...")
    
    # é€‰æ‹©è¿è¡Œæ¨¡å¼
    print("\né€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. ä»å·²æœ‰URLæ–‡ä»¶æ‰¹é‡æŠ“å– (æ¨è)")
    print("2. é‡æ–°çˆ¬å–URLç„¶åæŠ“å–å†…å®¹")
    
    choice = input("è¯·é€‰æ‹© (1 æˆ– 2): ").strip()
    
    if choice == "1":
        # ä»å·²æœ‰URLæŠ“å–
        start_index = int(input("èµ·å§‹ç´¢å¼• (é»˜è®¤0): ") or "0")
        max_urls = int(input("æœ€å¤§æŠ“å–æ•°é‡ (é»˜è®¤20): ") or "20")
        batch_crawl_from_urls(output_urls, output_json, start_index, max_urls)
    else:
        # é‡æ–°çˆ¬å–URL
        for start_path in START_PATHS:
            crawl_page(start_path)

        new_links = sorted(saved_links)
        with open(output_urls, "w", encoding="utf-8") as f:
            for url in new_links:
                f.write(url + "\n")

        print(f"\nâœ… å…±ä¿å­˜ {len(new_links)} æ¡é“¾æ¥åˆ°ï¼š{output_urls}")
        
        # ç„¶åæŠ“å–å†…å®¹
        if new_links:
            max_urls = int(input("æŠ“å–å¤šå°‘ä¸ªURLçš„å†…å®¹ (é»˜è®¤10): ") or "10")
            batch_crawl_from_urls(output_urls, output_json, 0, max_urls)

if __name__ == "__main__":
    run_full_process()
